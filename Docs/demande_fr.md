# Sp√©cification des besoins

## 1. Description des besoins du projet

### 1.1 Objectifs
- D√©velopper un perceptron multicouche (MLP) en C++ capable de reconna√Ætre les chiffres manuscrits MNIST.
- Construire un moteur de diff√©rentiation automatique en mode inverse pour g√©n√©rer automatiquement les gradients du mod√®le.
- Int√©grer l‚Äôensemble de la cha√Æne d‚Äôentra√Ænement (propagation avant, r√©tropropagation, SGD, fonctions d‚Äôactivation, initialisation) afin de livrer un solveur MNIST op√©rationnel √† la fin du premier semestre.
- Pr√©parer les bases n√©cessaires aux am√©liorations de performances et de pr√©cision pr√©vues pour le second semestre.

### 1.2 Livrables du projet
- Une biblioth√®que / un outil C++ fournissant une impl√©mentation MLP pour MNIST, comprenant le moteur d‚Äôautodiff√©rentiation, les algorithmes d‚Äôoptimisation essentiels et le pipeline de lecture des donn√©es.
- Une documentation technique assortie d‚Äôexemples d‚Äôutilisation pour guider l‚Äôentra√Ænement et l‚Äô√©valuation du mod√®le.

### 1.3 Fonctionnalit√©s du produit
- Propagation avant MLP configurable (nombre de couches, taille, fonctions d‚Äôactivation ajustables).
- Moteur d‚Äôautodiff√©rentiation en mode inverse permettant de g√©n√©rer le graphe de calcul et les gradients associ√©s.
- Prise en charge des op√©rations vecteur/matrice (produit matriciel, sommes √©l√©ment par √©l√©ment, produits √©l√©ment par √©l√©ment, activations √©l√©mentaires) et calcul de leurs d√©riv√©es partielles.
- Boucle d‚Äôentra√Ænement int√©grant la mise √† jour des poids/biais par SGD, une initialisation al√©atoire contr√¥lable et le suivi des performances.
- Int√©gration du processus de chargement des donn√©es MNIST avec une interface d‚Äôinf√©rence pour pr√©dire les chiffres manuscrits.

### 1.4 Crit√®res d‚Äôacceptation et de r√©ception
- Atteindre une pr√©cision au moins √©gale au seuil d√©fini sur le jeu de validation MNIST (exemple : ‚â• 90 % √† la fin du premier semestre).
- V√©rifier l‚Äôexactitude des gradients via une comparaison par diff√©rences num√©riques sur des cas de test s√©lectionn√©s.
- R√©pondre aux exigences de performance sur la plateforme cible (temps d‚Äôentra√Ænement raisonnable) et r√©ussir les tests unitaires des op√©rations critiques.
- Fournir une documentation compl√®te permettant d‚Äôex√©cuter le pipeline d‚Äôentra√Ænement et d‚Äôinf√©rence de bout en bout selon les instructions.

## 2. Contraintes

### 2.1 Contraintes de planning
- **08/11 Fin de la phase de conception**
  - Livrer l‚Äôarchitecture globale du syst√®me (d√©coupage en modules, flux de donn√©es, d√©pendances) ainsi que la documentation des interfaces.
  - Valider les choix technologiques (C++17, OpenBLAS, cadre de tests, etc.) et les scripts de mise en place de l‚Äôenvironnement.
  - Produire un plan d‚Äôit√©ration d√©taill√© et une liste des risques, valid√©s par la revue d‚Äô√©quipe.
- **16/11 Premi√®re version ex√©cutable**
  - Finaliser la propagation avant du MLP de base, le moteur d‚Äôautodiff√©rentiation en mode inverse et l‚Äôossature SGD/fonction de perte, puis les int√©grer.
  - R√©ussir les tests unitaires des op√©rations matricielles/vecteur et du calcul de gradients avec une erreur inf√©rieure √† 1e-5 par rapport aux diff√©rences num√©riques.
  - R√©aliser un cycle complet avant + arri√®re + mise √† jour des param√®tres sur un mini-batch MNIST √©chantillonn√©, avec une perte d√©croissante et un guide d‚Äôex√©cution.
- **07/12 Deuxi√®me version**
  - Int√©grer la cha√Æne d‚Äôentra√Ænement MNIST compl√®te (chargement des donn√©es, entra√Ænement mini-batch, enregistrement des m√©triques) avec une architecture de r√©seau configurable.
  - Atteindre ‚â• 90 % de pr√©cision sur le jeu de validation ou fournir les raisons de l‚Äô√©cart et le plan d‚Äôam√©lioration.
  - Produire un premier jet de la documentation (guide d‚Äôutilisation, rapport de tests, probl√®mes connus) pr√™t pour d√©monstration et livraison.

### 2.3 Contraintes mat√©rielles
- Ordinateur sous Linux avec VSCode comme environnement de d√©veloppement.

### 2.4 Autres contraintes
- C++
- Biblioth√®que OpenBLAS
- _√Ä compl√©ter : normes techniques, dispositions l√©gales, exigences de s√©curit√©, etc._

## 3. Mise en ≈ìuvre du projet

### 3.1 Planification
- **Premier semestre ‚Äî Impl√©mentation des fonctionnalit√©s cl√©s**
  1. D√©velopper un MLP de base disposant de la propagation avant.
  2. Construire un moteur de diff√©rentiation automatique en mode inverse couvrant les op√©rations vecteur/matrice (somme, produit) ainsi que les fonctions d‚Äôactivation √©l√©mentaires, y compris le calcul de leurs d√©riv√©es partielles.
  3. Int√©grer l‚Äôautodiff√©rentiation au MLP pour calculer les gradients.
  4. Mettre en place la boucle d‚Äôentra√Ænement incluant SGD (gestion des poids, biais, fonctions d‚Äôactivation).
  5. Int√©grer le pipeline MNIST complet en s‚Äôappuyant sur le code de lecture des donn√©es fourni.

- **Second semestre ‚Äî Optimisation des performances et de l‚Äôefficacit√©**
  - Am√©liorer la pr√©cision et la vitesse d‚Äôentra√Ænement (architecture du mod√®le, r√©gularisation, r√©glage des hyperparam√®tres, etc.).

### 3.2 Allocation des ressources
- Ressources humaines : 4 d√©veloppeurs C++ et un encadrant pour l‚Äôappui technique.
- Ressources mat√©rielles : environnement de d√©veloppement compatible C++17+, biblioth√®ques standard usuelles, puissance de calcul suffisante pour entra√Æner MNIST (CPU multi-c≈ìur, GPU optionnel).

# Projet ¬´ Autodiff + MLP + MNIST ¬ª

## üéØ Objectifs globaux
1. D√©velopper un MLP de base dot√© de la propagation avant.
2. Construire un moteur d‚Äôautodiff√©rentiation en mode inverse prenant en charge les op√©rations matricielles/vecteur et les fonctions d‚Äôactivation, avec calcul des d√©riv√©es.
3. Int√©grer l‚Äôautodiff√©rentiation au MLP pour r√©aliser le calcul des gradients.
4. Impl√©menter la boucle d‚Äôentra√Ænement incluant SGD, avec gestion des poids, biais et fonctions d‚Äôactivation.
5. Int√©grer le pipeline d‚Äôentra√Ænement et d‚Äô√©valuation MNIST √† l‚Äôaide du code fourni.

---

## üß© Phase 1 : Propagation avant du MLP de base

### 1. Structures de donn√©es et fondements math√©matiques
- [ ] Impl√©menter les classes `Matrix` / `Vector` (addition, multiplication, transposition, diffusion).
- [ ] Ajouter l‚Äôinitialisation al√©atoire et les fonctions d‚Äôaffichage.
- [ ] R√©diger des tests unitaires pour valider la justesse des op√©rations matricielles.

### 2. Module de fonctions d‚Äôactivation
- [ ] D√©finir la classe abstraite `ActivationFunction` (interface : `forward`, `backward`).
- [ ] Impl√©menter `ReLU`, `Sigmoid`, `Tanh`.
- [ ] √âcrire des tests pour v√©rifier la correction des sorties.

### 3. Module de couche lin√©aire
- [ ] D√©finir la classe `LinearLayer(in_dim, out_dim)`.
- [ ] Impl√©menter `forward(x) = x @ W + b`.
- [ ] Ajouter une initialisation al√©atoire (distribution gaussienne ou uniforme).
- [ ] Tester la coh√©rence des dimensions en entr√©e et en sortie.

### 4. Assemblage du MLP de base
- [ ] D√©finir la classe `MLPNetwork` combinant `Linear + Activation`.
- [ ] Impl√©menter `addLayer()` et `forward(input)`.
- [ ] V√©rifier manuellement le bon fonctionnement de la structure du r√©seau.

---

## ‚öôÔ∏è Phase 2 : Moteur de diff√©rentiation automatique

### 5. Fondamentaux des n≈ìuds et du graphe de calcul
- [ ] Impl√©menter la classe `Node` : `value`, `grad`, `parents`, `backward_fn`.
- [ ] Prendre en charge `backward()` avec un tri topologique automatique pour la r√©tropropagation du gradient.
- [ ] Tester les r√©sultats de la r√©tropropagation sur l‚Äôaddition / multiplication scalaires.

### 6. Prise en charge des op√©rations vecteur/matrice
- [ ] Impl√©menter `add`, `mul` (op√©rations √©l√©ment par √©l√©ment) et leurs r√®gles de r√©tropropagation.
- [ ] Impl√©menter `matmul` (produit matriciel) et les r√®gles inverses :
  - `dA += grad_output @ B·µÄ`
  - `dB += A·µÄ @ grad_output`
- [ ] V√©rifier la correction du gradient du produit matriciel.

### 7. Autodiff√©rentiation des fonctions d‚Äôactivation
- [ ] Enregistrer `relu`, `sigmoid`, `tanh` dans le cadre d‚Äôautodiff√©rentiation.
- [ ] Impl√©menter les r√®gles inverses correspondantes.
- [ ] Validar les r√©sultats par contr√¥le via gradient num√©rique.

### 8. Op√©rations d‚Äôagr√©gation
- [ ] Impl√©menter les op√©rations `sum`, `mean` et leur r√©tropropagation.
- [ ] Garantir la bonne diffusion (broadcast) des gradients.

---

## üîÅ Phase 3 : Int√©gration de l‚Äôautodiff√©rentiation dans le MLP

### 9. R√©√©criture de la propagation avant du MLP pour la construction du graphe
- [ ] Remplacer `Matrix` par `TensorNode`.
- [ ] Baser les op√©rations de chaque couche sur les n≈ìuds d‚Äôautodiff√©rentiation.

### 10. Calcul du gradient du MLP
- [ ] Appeler `loss.backward()` pour calculer automatiquement les gradients.
- [ ] Extraire `.grad` √† partir des n≈ìuds de param√®tres (W, b).

### 11. Validation des gradients
- [ ] Comparer les r√©sultats de l‚Äôautodiff√©rentiation par diff√©rences num√©riques.
- [ ] Tester la correction sur des fonctions simples (r√©gression lin√©aire).

---

## ‚öôÔ∏è Phase 4 : M√©canismes d‚Äôentra√Ænement (SGD + initialisation + activation)

### 12. Module de fonction de perte
- [ ] Impl√©menter `MSELoss(pred, target)`.
- [ ] (Optionnel) Impl√©menter `CrossEntropyLoss`.
- [ ] V√©rifier la justesse de la perte et de la r√©tropropagation.

### 13. Module d‚Äôoptimisation
- [ ] D√©finir la classe abstraite `Optimizer`.
- [ ] Impl√©menter `SGDOptimizer` (avec `step()` et `zero_grad()`).
- [ ] V√©rifier la tendance d√©croissante sur une fonction simple.

### 14. Strat√©gies d‚Äôinitialisation
- [ ] Impl√©menter les m√©thodes d‚Äôinitialisation `Xavier` et `Kaiming`.
- [ ] Choisir automatiquement la strat√©gie adapt√©e √† la fonction d‚Äôactivation.

### 15. Boucle d‚Äôentra√Ænement compl√®te
- [ ] D√©finir `train_step()` : propagation avant ‚Üí perte ‚Üí r√©tropropagation ‚Üí mise √† jour.
- [ ] Afficher la valeur de la perte √† chaque it√©ration.
- [ ] Confirmer la diminution continue de la perte durant l‚Äôentra√Ænement.

---

## üìä Phase 5 : Int√©gration du jeu de donn√©es MNIST

### 16. Chargement des donn√©es
- [ ] Utiliser les fonctions fournies pour charger les donn√©es MNIST.
- [ ] Normaliser les entr√©es sur [0,1] ou [-1,1].
- [ ] Impl√©menter un `DataLoader` (g√©n√©rateur de mini-batch).

### 17. D√©finition de l‚Äôarchitecture du r√©seau
- [ ] Construire la structure suivante :

---

784 ‚Üí 128 ‚Üí 64 ‚Üí 10  
Activation : ReLU  
Perte : CrossEntropy

---

- [ ] Initialiser les poids et les biais.

### 18. Entra√Ænement et √©valuation
- [ ] Entra√Æner plusieurs √©poques sur MNIST.
- [ ] Enregistrer les courbes de perte et de pr√©cision.
- [ ] √âvaluer la pr√©cision sur le jeu de test.

---

## üß† Phase 6 : Validation et documentation

### 19. V√©rification de la justesse
- [ ] Contr√¥ler l‚Äôabsence de gradients nuls/explosifs.
- [ ] Confirmer la diminution de la perte au fil des √©poques.
- [ ] √âvaluer la pr√©cision finale (objectif ‚â• 88‚Äì92 %).

### 20. Exp√©rimentations et analyses
- [ ] Tracer les courbes Loss/Accuracy.
- [ ] Comparer les effets des diff√©rentes initialisations et fonctions d‚Äôactivation.

### 21. Documentation et livraison
- [ ] R√©diger la documentation de conception et les diagrammes UML.
- [ ] Mettre √† jour le README avec les instructions d‚Äôex√©cution.
- [ ] Produire le rapport d‚Äôexp√©rimentation et les graphiques des r√©sultats.

---

